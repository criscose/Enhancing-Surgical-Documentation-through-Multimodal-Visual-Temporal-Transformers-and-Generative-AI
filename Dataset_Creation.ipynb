{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "mx5rQ-09qz_B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJ34IqLZqtg3"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import sys\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import functional as F\n",
        "\n",
        "import torch\n",
        "\n"
      ],
      "metadata": {
        "id": "Plq3yogEq3vY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update with your CholecT50 path location\n",
        "cholec_dir = \"/content/drive/My Drive/Master Thesis/CholecT50\"\n",
        "save_dir = \"path/to/save\"\n",
        "\n",
        "video_dir = cholec_dir + \"/videos\"\n",
        "json_annotations = cholec_dir + \"/labels/VID01.json\"\n",
        "labels_dir = cholec_dir + \"/labels\"\n"
      ],
      "metadata": {
        "id": "GHnLBNs_rCj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Frame Dateset Creation\n",
        "\n",
        "This section covers the necessary steps for the frame datasets, which will be used for the frame-level models, i.e Obect detection and Frame caption generation. Due to sorage capacity reasons, the 50 videos are treated and stored by group of 10, leading to 5 processed frame datasets."
      ],
      "metadata": {
        "id": "fsyLnwo-rkHA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions"
      ],
      "metadata": {
        "id": "BG-iqJ6NrFnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Mapping dictionaries\n",
        "instrument_mapping = {0: \"grasper\", 1: \"bipolar\", 2: \"hook\", 3: \"scissors\", 4: \"clipper\", 5: \"irrigator\", 6: \"specimen_bag\", 7: \"no_instrument\"}\n",
        "verb_mapping = {0: \"grasp\", 1: \"retract\", 2: \"dissect\", 3: \"coagulate\", 4: \"clip\", 5: \"cut\", 6: \"aspirate\", 7: \"irrigate\", 8: \"pack\", 9: \"null_verb\"}\n",
        "target_mapping = {0: \"gallbladder\", 1: \"cystic_plate\", 2: \"cystic_duct\", 3: \"cystic_artery\", 4: \"cystic_pedicle\", 5: \"blood_vessel\",\n",
        "                  6: \"fluid\", 7: \"abdominal_wall_cavity\", 8: \"liver\", 9: \"adhesion\", 10: \"omentum\", 11: \"peritoneum\",\n",
        "                  12: \"gut\", 13: \"specimen_bag\", 14: \"null_target\"}\n",
        "phase_mapping = {0: \"preparation\", 1: \"calot-triangle-dissection\", 2: \"clipping-and-cutting\", 3: \"gallbladder-dissection\",\n",
        "                 4: \"gallbladder-packaging\", 5: \"cleaning-and-coagulation\", 6: \"gallbladder-extraction\"}\n",
        "\n",
        "\n",
        "mapping = {0: \"grasper\", 1: \"bipolar\", 2: \"hook\", 3: \"scissors\", 4: \"clipper\", 5: \"irrigator\", 6: \"specimen_bag\", 7: \"gallbladder\",\n",
        "           8: \"cystic_plate\", 9: \"cystic_duct\", 10: \"cystic_artery\", 11: \"cystic_pedicle\", 12: \"blood_vessel\", 13: \"fluid\",\n",
        "           14: \"abdominal_wall_cavity\", 15: \"liver\", 16: \"adhesion\", 17: \"omentum\", 18: \"peritoneum\", 19: \"gut\", 20: \"specimen_bag\", 21: \"null_target\"}\n",
        "reverse_mapping = {v: k for k, v in mapping.items()}"
      ],
      "metadata": {
        "id": "0wlRi7xUrGwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3krsStjRevp"
      },
      "outputs": [],
      "source": [
        "# Turn Json vectors into binary vector of size 21 for instrument/target presence\n",
        "def decode_vector(vectors):\n",
        "  result = np.zeros(21)\n",
        "  for vector in vectors:\n",
        "    idx = int(vector[0])\n",
        "    if idx == -1:\n",
        "      return result\n",
        "    with open(json_annotations, 'r') as file:\n",
        "      data = json.load(file)\n",
        "    triplets = data['categories']['triplet']\n",
        "    triplet = triplets[str(idx)]\n",
        "    instrument, verb, target = triplet.split(',')\n",
        "    instrument_number = reverse_mapping.get(instrument)\n",
        "    target_number = reverse_mapping.get(target)\n",
        "\n",
        "    result[instrument_number] = 1\n",
        "    if target_number != 21:\n",
        "      result[target_number] = 1\n",
        "\n",
        "  return result\n",
        "\n",
        "# Create the sentence with the annotations from the json\n",
        "def sentence(annotations):\n",
        "    n = len(annotations)\n",
        "    i = 1\n",
        "    for vector in annotations:\n",
        "        t = vector[0]\n",
        "        p = vector[-1]\n",
        "        if t==-1:\n",
        "          answer = \"Unknown\"\n",
        "          return answer\n",
        "        with open(json_annotations, 'r') as file:\n",
        "            data = json.load(file)\n",
        "        triplets = data['categories']['triplet']\n",
        "        triplet = triplets[str(t)]\n",
        "        instrument, verb, target = triplet.split(',')\n",
        "        phases = data['categories']['phase']\n",
        "        phase = phases[str(p)]\n",
        "\n",
        "        print(f\"The {instrument} is {verb}ing the {target}\", end=\"\")\n",
        "\n",
        "        if i < n:\n",
        "            print(\", and \", end=\"\")\n",
        "        if i == n:\n",
        "            print(f\" during phase {phase}.\", end=\"\")\n",
        "        i += 1\n",
        "    print()\n",
        "\n",
        "\n",
        "def sentence_return(annotations):\n",
        "    n = len(annotations)\n",
        "    i = 1\n",
        "    result = \"\"\n",
        "\n",
        "    for vector in annotations:\n",
        "        t = vector[0]\n",
        "        p = vector[-1]\n",
        "\n",
        "        if t == -1:\n",
        "            answer = \"Unknown\"\n",
        "            return answer\n",
        "        with open(json_annotations, 'r') as file:\n",
        "            data = json.load(file)\n",
        "        triplets = data['categories']['triplet']\n",
        "        triplet = triplets[str(t)]\n",
        "        instrument, verb, target = triplet.split(',')\n",
        "        phases = data['categories']['phase']\n",
        "        phase = phases[str(p)]\n",
        "\n",
        "        if i == 1:\n",
        "          result += f\" During phase {phase}, \"\n",
        "\n",
        "        if verb != \"null_verb\":\n",
        "            if verb[-1] == \"e\":\n",
        "              verb = verb[:-1]\n",
        "            if instrument == \"scissors\":\n",
        "                result += f\"the {instrument} are {verb}ing the {target}\"\n",
        "            else:\n",
        "              result += f\"the {instrument} is {verb}ing the {target}\"\n",
        "        else:\n",
        "            result += f\"the {instrument} is present\"\n",
        "\n",
        "        if i < n:\n",
        "            result += \", \"\n",
        "        i += 1\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def get_objects(annotations,json_annotations):\n",
        "    n = len(annotations)\n",
        "    i = 1\n",
        "\n",
        "    objects = []\n",
        "    for vector in annotations:\n",
        "        t = vector[0]\n",
        "        p = vector[-1]\n",
        "\n",
        "        if t==-1:\n",
        "          answer = \"Unknown\"\n",
        "          return answer\n",
        "        with open(json_annotations, 'r') as file:\n",
        "            data = json.load(file)\n",
        "        triplets = data['categories']['triplet']\n",
        "        triplet = triplets[str(t)]\n",
        "        instrument, verb, target = triplet.split(',')\n",
        "\n",
        "        if instrument != \"null_instrument\":\n",
        "          objects.append(str(instrument))\n",
        "        if target != \"null_target\":\n",
        "          objects.append(str(target))\n",
        "\n",
        "    return objects\n",
        "\n",
        "def description(video, frame):\n",
        "    base_json_path = labels_dir\n",
        "    video_folder = f\"VID{video:02d}\"\n",
        "    json_path = f\"{base_json_path}{video_folder}.json\"\n",
        "    try:\n",
        "        with open(json_path, 'r') as file:\n",
        "            data = json.load(file)\n",
        "        frame_annotations = data[\"annotations\"].get(str(frame), \"No annotations found for this frame.\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"Not a Video\")\n",
        "        return None\n",
        "    return sentence_return(frame_annotations)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdBV10DQpDYY"
      },
      "outputs": [],
      "source": [
        "def get_objects(annotations,json_annotations):\n",
        "    n = len(annotations)\n",
        "    i = 1\n",
        "\n",
        "    objects = []\n",
        "    for vector in annotations:\n",
        "        t = vector[0]\n",
        "        p = vector[-1]\n",
        "\n",
        "        if t==-1:\n",
        "          answer = \"Unknown\"\n",
        "          return answer\n",
        "        with open(json_annotations, 'r') as file:\n",
        "            data = json.load(file)\n",
        "        triplets = data['categories']['triplet']\n",
        "        triplet = triplets[str(t)]\n",
        "        instrument, verb, target = triplet.split(',')\n",
        "\n",
        "        if instrument != \"null_instrument\":\n",
        "          objects.append(str(instrument))\n",
        "        if target != \"null_target\":\n",
        "          objects.append(str(target))\n",
        "\n",
        "    return objects\n",
        "\n",
        "\n",
        "def get_frame_caption(annotations, json_annotations):\n",
        "    n = len(annotations)\n",
        "    i = 1\n",
        "    result = \"\"\n",
        "\n",
        "    for vector in annotations:\n",
        "        t = vector[0]\n",
        "        p = vector[-1]\n",
        "\n",
        "        if t == -1:\n",
        "            answer = \"Unknown\"\n",
        "            return answer\n",
        "        with open(json_annotations, 'r') as file:\n",
        "            data = json.load(file)\n",
        "        triplets = data['categories']['triplet']\n",
        "        triplet = triplets[str(t)]\n",
        "        instrument, verb, target = triplet.split(',')\n",
        "        phases = data['categories']['phase']\n",
        "        phase = phases[str(p)]\n",
        "\n",
        "        if i == 1:\n",
        "          result += f\" During phase {phase}, \"\n",
        "\n",
        "        if verb != \"null_verb\":\n",
        "            if verb[-1] == \"e\":\n",
        "              verb = verb[:-1]\n",
        "            if instrument == \"scissors\":\n",
        "                result += f\"the {instrument} are {verb}ing the {target}\"\n",
        "            else:\n",
        "              result += f\"the {instrument} is {verb}ing the {target}\"\n",
        "        else:\n",
        "            result += f\"the {instrument} is present\"\n",
        "\n",
        "        if i < n:\n",
        "            result += \", \"\n",
        "        i += 1\n",
        "\n",
        "    return result\n",
        "\n",
        "def preprocess_frame(frame_path, target_size=(224, 224)):\n",
        "  transform = transforms.Compose([\n",
        "        transforms.Resize(target_size),\n",
        "        transforms.ToTensor(),\n",
        "  ])\n",
        "\n",
        "  image = Image.open(frame_path).convert('RGB')\n",
        "  tensor = transform(image)\n",
        "\n",
        "  return tensor\n",
        "\n",
        "\n",
        "\n",
        "def annotation_to_label(annotations, json_dir):\n",
        "  labels = np.zeros(21)\n",
        "  with open(json_dir, 'r') as file:\n",
        "      data = json.load(file)\n",
        "  for vector in annotations:\n",
        "    idx = int(vector[0])\n",
        "    if idx == -1:\n",
        "      return labels\n",
        "    triplets = data['categories']['triplet']\n",
        "    triplet = triplets[str(idx)]\n",
        "    instrument, verb, target = triplet.split(',')\n",
        "    instrument_number = reverse_mapping.get(instrument)\n",
        "    target_number = reverse_mapping.get(target)\n",
        "    labels[instrument_number] = 1\n",
        "    if target_number != 21:\n",
        "      labels[target_number] = 1\n",
        "  return labels\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create datasets"
      ],
      "metadata": {
        "id": "CoqPMDySsvUs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDkIGIf_oZ5h"
      },
      "outputs": [],
      "source": [
        "def create_dataset(video_dir, labels_dir, json_annotations, start_video=None, end_video=None):\n",
        "\n",
        "\n",
        "    dataset = []\n",
        "\n",
        "    all_video_folders = sorted(os.listdir(video_dir))\n",
        "    if start_video is not None or end_video is not None:\n",
        "        all_video_folders = all_video_folders[start_video:end_video]\n",
        "\n",
        "    for video_folder in all_video_folders:\n",
        "        print(video_folder)\n",
        "        video_path = os.path.join(video_dir, video_folder)\n",
        "\n",
        "        if not os.path.isdir(video_path):\n",
        "            continue\n",
        "\n",
        "        frames = sorted(os.listdir(video_path))\n",
        "        frames = [f for f in frames if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "        processed_frames = set()\n",
        "\n",
        "        idx = 0\n",
        "        while idx < len(frames):\n",
        "            frame_name = frames[idx]\n",
        "            # Skip duplicate frames\n",
        "            if frame_name in processed_frames:\n",
        "                idx += 1\n",
        "                continue\n",
        "\n",
        "            processed_frames.add(frame_name)\n",
        "            match = re.match(r'^(\\d+)', os.path.splitext(frame_name)[0])\n",
        "            if not match:\n",
        "                print(f\"Skipping invalid frame name: {frame_name}\")\n",
        "                idx += 1\n",
        "                continue\n",
        "\n",
        "            frame_number = int(match.group(1))\n",
        "\n",
        "            frame_path = os.path.join(video_path, frame_name)\n",
        "            video_id = os.path.basename(os.path.dirname(frame_path))\n",
        "            annotation_file = os.path.join(labels_dir, f\"{video_id}.json\")\n",
        "\n",
        "            with open(annotation_file, \"r\") as f:\n",
        "                annotations = json.load(f)\n",
        "\n",
        "            frame_key = str(frame_number)\n",
        "            frame_annotation = annotations[\"annotations\"].get(frame_key, None)\n",
        "            frame_name = frame_name.replace(\".png\", \"\")\n",
        "\n",
        "            objects = get_objects(frame_annotation, json_annotations)\n",
        "            frame_caption = get_frame_caption(frame_annotation, json_annotations)\n",
        "            frame = preprocess_frame(frame_path)\n",
        "            object_labels = annotation_to_label(frame_annotation, json_annotations)\n",
        "\n",
        "            if frame_caption != \"Unknown\":\n",
        "                dataset.append({\n",
        "                    \"video\": video_folder,\n",
        "                    \"frame_number\": frame_name,\n",
        "                    \"frame\": frame,\n",
        "                    \"object_labels\": object_labels,\n",
        "                    \"objects\": objects,\n",
        "                    \"frame_caption\": frame_caption\n",
        "                })\n",
        "\n",
        "            idx += 1\n",
        "\n",
        "    return dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzS8KHJJDkA5",
        "outputId": "e74d38f4-07b2-4ee8-b3d1-22a26988206f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VID68\n",
            "VID70\n",
            "VID73\n",
            "VID74\n",
            "VID75\n",
            "VID78\n",
            "VID79\n",
            "VID80\n",
            "VID92\n",
            "VID96\n"
          ]
        }
      ],
      "source": [
        "start = 40\n",
        "end = 50\n",
        "\n",
        "\n",
        "dataset = create_dataset(video_dir, labels_dir, json_annotations, start, end)\n",
        "torch.save(dataset, f\"{save_dir}/Datasets/frame_dataset_{start}_{end-1}.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clip Dataset Creation\n",
        "\n",
        "creating and preprocessing the dataset necessary for the clip caption generation model. Same as before, the videos are treated 10 by 10."
      ],
      "metadata": {
        "id": "WgTYCwudtGeM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions"
      ],
      "metadata": {
        "id": "sQjCsPxFuNvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def extract_phase_and_actions(captions):\n",
        "    results = []\n",
        "    prev_phase = None\n",
        "    grouped_actions = []\n",
        "    time_count = 0\n",
        "\n",
        "    for caption in captions:\n",
        "        phase_match = re.search(r'during phase ([a-zA-Z\\-]+)', caption, re.IGNORECASE)\n",
        "        phase = phase_match.group(1) if phase_match else prev_phase\n",
        "\n",
        "        if phase is None:\n",
        "            continue\n",
        "\n",
        "        actions_part = re.sub(r'during phase [a-zA-Z\\-]+,? ', '', caption, flags=re.IGNORECASE)\n",
        "        actions = [action.strip() for action in actions_part.split(',') if action.strip() and action.lower() != \"unknown\"]\n",
        "\n",
        "        if phase == prev_phase:\n",
        "            grouped_actions = list(set(grouped_actions + actions))\n",
        "            time_count += 1\n",
        "            results[-1][\"actions\"] = grouped_actions\n",
        "            results[-1][\"time\"] = time_count\n",
        "        else:\n",
        "            grouped_actions = actions\n",
        "            time_count = 1\n",
        "            results.append({\"phase\": phase, \"actions\": grouped_actions, \"time\": time_count})\n",
        "\n",
        "        prev_phase = phase\n",
        "\n",
        "    return create_sentence(results)\n",
        "\n",
        "\n",
        "def create_sentence(results):\n",
        "    if not results:\n",
        "        return \"\"\n",
        "\n",
        "    sentence = \"\"\n",
        "    phase_count = len(results)\n",
        "    connectors = [\"First\", \"Then\", \"Then\", \"Then\", \"Finally\"]\n",
        "\n",
        "    for i, entry in enumerate(results):\n",
        "        phase = entry[\"phase\"]\n",
        "        time = entry[\"time\"]\n",
        "        actions = entry[\"actions\"]\n",
        "\n",
        "        if len(actions) == 1:\n",
        "            actions_text = actions[0]\n",
        "        elif len(actions) == 2:\n",
        "            actions_text = f\"{actions[0]} while {actions[1]}\"\n",
        "        else:\n",
        "            actions_text = f\"{', '.join(actions[:-1])} and {actions[-1]}\"\n",
        "\n",
        "        if phase_count == 1:\n",
        "            sentence += f\"During the phase of {phase} lasting {time} seconds, {actions_text}.\"\n",
        "        else:\n",
        "            connector = connectors[min(i, len(connectors) - 1)]\n",
        "            sentence += f\" {connector}, during the phase of {phase} lasting {time} seconds, {actions_text}.\"\n",
        "\n",
        "    return sentence.strip()\n",
        "\n",
        "\n",
        "\n",
        "# Constructs the frame caption from json annotations\n",
        "def sentence_return(annotations):\n",
        "    n = len(annotations)\n",
        "    i = 1\n",
        "    result = \"\"\n",
        "\n",
        "    for vector in annotations:\n",
        "        t = vector[0]\n",
        "        p = vector[-1]\n",
        "\n",
        "        if t == -1:\n",
        "            answer = \"Unknown\"\n",
        "            return answer\n",
        "        with open(json_annotations, 'r') as file:\n",
        "            data = json.load(file)\n",
        "        triplets = data['categories']['triplet']\n",
        "        triplet = triplets[str(t)]\n",
        "        instrument, verb, target = triplet.split(',')\n",
        "        phases = data['categories']['phase']\n",
        "        phase = phases[str(p)]\n",
        "\n",
        "        if i == 1:\n",
        "          result += f\" During phase {phase}, \"\n",
        "\n",
        "        if verb != \"null_verb\":\n",
        "            if verb[-1] == \"e\":\n",
        "              verb = verb[:-1]\n",
        "            if instrument == \"scissors\":\n",
        "                result += f\"the {instrument} are {verb}ing the {target}\"\n",
        "            else:\n",
        "              result += f\"the {instrument} is {verb}ing the {target}\"\n",
        "        else:\n",
        "            result += f\"the {instrument} is present\"\n",
        "\n",
        "        if i < n:\n",
        "            result += \", \"\n",
        "        i += 1\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_frame_captions(frame_paths):\n",
        "    captions = []\n",
        "    for frame_path in frame_paths:\n",
        "        video_id = os.path.basename(os.path.dirname(frame_path))\n",
        "        frame_number = os.path.splitext(os.path.basename(frame_path))\n",
        "        frame_number = int(frame_number[0][:6])\n",
        "\n",
        "        annotation_file = os.path.join(labels_dir, f\"{video_id}.json\")\n",
        "\n",
        "        with open(annotation_file, \"r\") as f:\n",
        "            annotations = json.load(f)\n",
        "\n",
        "        frame_key = str(frame_number)\n",
        "        frame_annotation = annotations[\"annotations\"].get(frame_key, None)\n",
        "        caption = sentence_return(frame_annotation)\n",
        "        captions.append(caption)\n",
        "    return captions\n",
        "\n",
        "\n",
        "# Reshape Frames\n",
        "def load_and_preprocess_frames(frame_paths, target_size=(224, 224)):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(target_size),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    frames = []\n",
        "    for path in frame_paths:\n",
        "        image = Image.open(path).convert('RGB')\n",
        "        tensor = transform(image).to(device)\n",
        "        frames.append(tensor)\n",
        "\n",
        "    video_tensor = torch.stack(frames).to(device)\n",
        "    return video_tensor\n"
      ],
      "metadata": {
        "id": "0Q4aRMhIR9M5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create"
      ],
      "metadata": {
        "id": "qw4eb9TZuKDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "def create_dataset(device, video_dir, clip_length, overlap, start_idx, end_idx):\n",
        "    dataset = []\n",
        "\n",
        "    video_folders = sorted([vf for vf in os.listdir(video_dir) if os.path.isdir(os.path.join(video_dir, vf))])\n",
        "    selected_videos = video_folders[start_idx:end_idx + 1]\n",
        "\n",
        "    for video_folder in selected_videos:\n",
        "        print(f\"Processing: {video_folder}\")\n",
        "        video_path = os.path.join(video_dir, video_folder)\n",
        "\n",
        "        frames = sorted([f for f in os.listdir(video_path) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
        "        start_frame_idx = 0\n",
        "\n",
        "        while start_frame_idx + clip_length <= len(frames):\n",
        "            frame_paths = [\n",
        "                os.path.join(video_path, frames[i])\n",
        "                for i in range(start_frame_idx, start_frame_idx + clip_length)\n",
        "            ]\n",
        "\n",
        "            frame_numbers = [path.split('/')[-1].split('.')[0] for path in frame_paths]\n",
        "            clip = load_and_preprocess_frames(frame_paths).to(device)\n",
        "            frame_captions = get_frame_captions(frame_paths)\n",
        "            clip_caption = extract_phase_and_actions(frame_captions)\n",
        "\n",
        "            dataset.append({\n",
        "                \"video\": video_folder,\n",
        "                \"frame_numbers\": frame_numbers,\n",
        "                \"clip\": clip,\n",
        "                \"frame_captions\": frame_captions,\n",
        "                \"clip_caption\": clip_caption\n",
        "            })\n",
        "\n",
        "            start_frame_idx += clip_length - overlap\n",
        "\n",
        "    return dataset\n"
      ],
      "metadata": {
        "id": "WjSU2WziqR0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKwm_x0L9sIY",
        "outputId": "d100ba4e-0760-454e-eec7-ee347756bb5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing: VID68\n",
            "Processing: VID70\n",
            "Processing: VID73\n",
            "Processing: VID74\n",
            "Processing: VID75\n",
            "Processing: VID78\n",
            "Processing: VID79\n",
            "Processing: VID80\n",
            "Processing: VID92\n",
            "Processing: VID96\n",
            "Dataset saved!\n"
          ]
        }
      ],
      "source": [
        "clip_length = 32\n",
        "overlap = 16\n",
        "\n",
        "start = 40\n",
        "end = 49\n",
        "\n",
        "dataset = create_dataset(device,video_dir, clip_length, overlap, start, end)\n",
        "torch.save(dataset, f\"{save_dir}/clip_dataset_{start}_{end}.pt\")\n",
        "print(\"Dataset saved!\")"
      ]
    }
  ]
}