{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxEwjrsb3-qC"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1SHiY073QLL",
        "outputId": "3fedbd44-4bf7-47f4-ec6b-b6ca962cf8e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score\n",
        "!pip install bert_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8k13x_l3rfQ1",
        "outputId": "c4e519bb-30d5-4589-9045-f3b1c32c6380"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=933c87a45a5c033d6ba24c43fe460becc7e9d65301f0255175aa56a312812125\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n",
            "Collecting bert_score\n",
            "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.5.1+cu124)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.2.2)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.48.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bert_score) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.67.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert_score) (3.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert_score) (24.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.0.0->bert_score)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.0.0->bert_score)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.0.0->bert_score)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.0.0->bert_score)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.0.0->bert_score)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.0.0->bert_score)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.0.0->bert_score)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.0.0->bert_score)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.0.0->bert_score)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.0.0->bert_score)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert_score) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.28.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.5.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (3.2.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2025.1.31)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\n",
            "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m101.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bert_score\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed bert_score-0.3.13 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Z4eoq0i62p3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms, models\n",
        "\n",
        "from transformers import (\n",
        "    ViTModel,\n",
        "    BertModel,\n",
        "    BertTokenizer,\n",
        "    AutoModel,\n",
        "    AutoTokenizer,\n",
        "    T5ForConditionalGeneration,\n",
        "    T5Tokenizer,\n",
        "    DetrForObjectDetection,\n",
        "    DetrImageProcessor,\n",
        "    VivitModel,\n",
        "    AdamW,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "from transformers.modeling_outputs import BaseModelOutput\n",
        "from transformers import logging\n",
        "\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from bert_score import score as bert_score\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "Prio41cS0RPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdxAxIQG5Zlw"
      },
      "outputs": [],
      "source": [
        "# Update with your own save path\n",
        "save_dir = \"/content/drive/My Drive/Master Thesis/CholecT50\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3kLyKotz7s0"
      },
      "source": [
        "# Load datasets and create dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAWo4fHZ0lMn",
        "outputId": "1fa0c542-e0b9-4c3f-83bd-aa0d83689128"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n"
          ]
        }
      ],
      "source": [
        "dataset = []\n",
        "for i in range(5):\n",
        "  d =  torch.load(f\"{save_dir}/Datasets/frame_dataset_{int(i*10)}_{(int(i+1)*10)}.pt\")\n",
        "  print(i)\n",
        "  dataset.extend(d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnOY-jEuAJUQ",
        "outputId": "e1494fd4-45d1-417c-974d-5fb689f9fdc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['video', 'frame_number', 'frame', 'object_labels', 'objects', 'frame_caption'])\n",
            "89827\n",
            "<class 'str'>  During phase preparation, the grasper is grasping the gallbladder\n",
            "<class 'torch.Tensor'> torch.Size([3, 224, 224])\n",
            "<class 'list'> ['grasper', 'gallbladder']\n"
          ]
        }
      ],
      "source": [
        "print(dataset[0].keys())\n",
        "print(len(dataset))\n",
        "print(type(dataset[0][\"frame_caption\"]), dataset[0][\"frame_caption\"])\n",
        "print(type(dataset[0][\"frame\"]), dataset[0][\"frame\"].shape)\n",
        "print(type(dataset[0][\"objects\"]), dataset[0][\"objects\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    max_objects = 10\n",
        "    return {\n",
        "        'video': [item['video'] for item in batch],\n",
        "        'frame_number': [item['frame_number'] for item in batch],\n",
        "        'frame': torch.stack([item['frame'] for item in batch]),\n",
        "        'frame_caption': [item['frame_caption'] for item in batch],\n",
        "        'objects': [\n",
        "            item['objects'] + [''] * (max_objects - len(item['objects'])) if len(item['objects']) < max_objects\n",
        "            else item['objects'][:max_objects] for item in batch\n",
        "        ]\n",
        "    }"
      ],
      "metadata": {
        "id": "IEfVZzQpRlUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhiNks2aQRuD"
      },
      "outputs": [],
      "source": [
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = int(0.1 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = random_split(\n",
        "    dataset, [train_size, val_size, test_size],\n",
        "    generator=torch.Generator().manual_seed(42)\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "batch_size = 8\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-uDd7Dqanlx"
      },
      "source": [
        "# First Training\n",
        "\n",
        "This section initializes the model and trains the model using the ground-truth objects for the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model"
      ],
      "metadata": {
        "id": "OgKbnlmQ03V0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtdParV3toVG"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "class FrameCaptioner(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Frame encoder\n",
        "        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "        self.video_proj = nn.Linear(768, 512)\n",
        "\n",
        "        # Object encoder\n",
        "        self.text_encoder = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.text_tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.text_proj = nn.Linear(768, 512)\n",
        "\n",
        "        # Text decoder\n",
        "        self.decoder = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "        self.decoder_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "    def forward(self, frame, objects, frame_caption=None):\n",
        "        batch_size = frame.shape[0]\n",
        "\n",
        "        # frame\n",
        "        frame_features = self.vit(frame).last_hidden_state\n",
        "        frame_features = self.video_proj(frame_features)\n",
        "\n",
        "        # object\n",
        "        object_texts = [\" \".join(obj_list) for obj_list in objects]\n",
        "        tokenized_objects = self.text_tokenizer(\n",
        "            object_texts, padding=True, truncation=True, return_tensors=\"pt\"\n",
        "        ).to(frame.device)\n",
        "        text_features = self.text_encoder(**tokenized_objects).last_hidden_state\n",
        "        text_features = self.text_proj(text_features)\n",
        "\n",
        "        # fuse\n",
        "        combined_features = torch.cat((frame_features, text_features), dim=1)\n",
        "\n",
        "        # training\n",
        "        if frame_caption is not None:\n",
        "            target_ids = self.decoder_tokenizer(\n",
        "                frame_caption, padding=True, truncation=True, return_tensors=\"pt\"\n",
        "            ).input_ids.to(frame.device)\n",
        "\n",
        "            outputs = self.decoder(\n",
        "                encoder_outputs=(combined_features,),\n",
        "                labels=target_ids\n",
        "            )\n",
        "            print(outputs.logits.shape)\n",
        "            return outputs\n",
        "        # generation\n",
        "        else:\n",
        "            input_ids = torch.ones(batch_size, 1).fill_(self.decoder_tokenizer.pad_token_id).to(frame.device)\n",
        "            output = self.decoder.generate(\n",
        "                input_ids=input_ids,\n",
        "                encoder_outputs=BaseModelOutput(last_hidden_state=combined_features),\n",
        "                max_length=64,\n",
        "                temperature=0.2,\n",
        "                top_k=10,\n",
        "                top_p=0.7,\n",
        "                do_sample=True,\n",
        "                no_repeat_ngram_size=2,\n",
        "            )\n",
        "            generated_captions = [self.decoder_tokenizer.decode(seq, skip_special_tokens=True) for seq in output]\n",
        "            return generated_captions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anwKnpgcUDnj"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def run_epoch(model, loader, optimizer=None, train=True, temperature=1.0):\n",
        "    mode = \"Training\" if train else \"Validation\"\n",
        "    model.train() if train else model.eval()\n",
        "    total_loss = 0\n",
        "    total_batches = len(loader)\n",
        "\n",
        "    for batch_idx, batch in enumerate(loader):\n",
        "        frame = batch['frame'].to(device)\n",
        "        target = batch['frame_caption']\n",
        "        objects = batch['objects']\n",
        "\n",
        "        if train:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(frame, objects, target)\n",
        "            loss = outputs.loss / temperature\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            if scheduler:\n",
        "                scheduler.step()\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                outputs = model(frame, objects, target)\n",
        "                loss = outputs.loss / temperature\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Print progress bar\n",
        "        progress = (batch_idx + 1) / total_batches\n",
        "        bar_length = 20\n",
        "        filled_length = int(bar_length * progress)\n",
        "        bar = \"=\" * filled_length + \" \" * (bar_length - filled_length)\n",
        "        percentage = int(progress * 100)\n",
        "        sys.stdout.write(f\"\\r[{bar}] {percentage}% - Batch {batch_idx+1}/{total_batches} - Loss: {loss.item():.4f} - Avg {mode} Loss: {total_loss / (batch_idx + 1):.4f}\")\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    print()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "\n",
        "# Initialize\n",
        "model = FrameCaptioner().to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "num_epochs = 15\n",
        "temperature = 2.0\n",
        "num_training_steps = len(train_loader) * num_epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
        "\n",
        "\n",
        "# Train loop\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}: \")\n",
        "    loss = run_epoch(model, train_loader, optimizer, train=True, temperature=temperature)\n",
        "    val_loss = run_epoch(model, val_loader, train=False, temperature=temperature)\n",
        "    #torch.save(model.state_dict(), f\"{save_dir}/Models/model_FC.pth\")"
      ],
      "metadata": {
        "id": "VPV0QmxUbOu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUZ7RtSsUFS2"
      },
      "source": [
        "## Print examples"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = FrameCaptioner().to(device)\n",
        "model.load_state_dict(torch.load(f\"{save_dir}/Models/model_FC.pth\"))"
      ],
      "metadata": {
        "id": "vBs3vIFy1hor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00Xy24Fp5ztb",
        "outputId": "6f0d61cc-3aee-44d2-eb4e-4b47e9f25598"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: During phase carlot-triangle-dissection, the grasper is grasping the gallbladder, their bipolar is dissecting the cystic_artery\n",
            "Target:  During phase carlot-triangle-dissection, the bipolar is dissecting the cystic_artery, the grasper is grasping the gallbladder\n",
            "==================================================\n",
            "Predicted: During phase gallbladder-packaging, the grasper is grasping the specimen_bag\n",
            "Target:  During phase gallbladder-packaging, the grasper is grasping the specimen_bag\n",
            "==================================================\n",
            "Predicted: During phase gallbladder-dissection, the hook is dissecting the gallbloddger\n",
            "Target:  During phase gallbladder-dissection, the hook is dissecting the gallbladder\n",
            "==================================================\n",
            "Predicted: During phase carlot-triangle-dissection, the grasper is retracting the gallbladder, this hook is present\n",
            "Target:  During phase carlot-triangle-dissection, the grasper is retracting the gallbladder, the hook is present\n",
            "==================================================\n",
            "Predicted: During phase gallbladder-dissection, the grasper is retracting the gallbleddger, this hook is present\n",
            "Target:  During phase gallbladder-dissection, the grasper is retracting the gallbladder, the hook is present\n",
            "==================================================\n",
            "Predicted: During phase gallbladder-dissection, the grasper is grasping the gallbloddner, this clipper is cliping your cystic_artery\n",
            "Target:  During phase gallbladder-dissection, the grasper is grasping the gallbladder, the clipper is cliping the cystic_artery\n",
            "==================================================\n",
            "Predicted: During phase gallbladder-dissection, the hook is dissecting the gallbloddger\n",
            "Target:  During phase gallbladder-dissection, the hook is dissecting the gallbladder\n",
            "==================================================\n",
            "Predicted: During phase gallbladder-dissection, the hook is dissecting the gallbludger\n",
            "Target:  During phase gallbladder-dissection, the hook is dissecting the gallbladder\n",
            "==================================================\n",
            "Predicted: During phase carlot-triangle-dissection, the grasper is retracting the gallbladder, those bipolar is coagulating the abdominal_wall_cavity\n",
            "Target:  During phase carlot-triangle-dissection, the grasper is retracting the gallbladder, the bipolar is coagulating the abdominal_wall_cavity\n",
            "==================================================\n",
            "Predicted: During phase carlot-triangle-dissection, the grasper is retracting the gallbladder, this hook is dissecting the omentum\n",
            "Target:  During phase carlot-triangle-dissection, the grasper is retracting the gallbladder, the hook is dissecting the omentum\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "def print_examples(model, loader, device, num_examples = 5):\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        i = 1\n",
        "        for batch in test_loader:\n",
        "\n",
        "            frame = batch['frame'].to(device)\n",
        "            objects = batch['objects']\n",
        "            target = batch['frame_caption']\n",
        "\n",
        "            predicted_caption = model(frame, objects)\n",
        "\n",
        "            print(f\"Predicted: {predicted_caption}\")\n",
        "            print(f\"Target: {target[0]}\")\n",
        "            print(\"=\"*50)\n",
        "            i += 1\n",
        "            if i > num_examples:\n",
        "                break\n",
        "\n",
        "print_examples(model, test_loader, device, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train with Detected Objects\n",
        "\n",
        "The trained model is loaded and trained again using the objects detected by the OD model stored in predicted_objects.json"
      ],
      "metadata": {
        "id": "-zH18-6n-Uw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f\"{save_dir}/Predictions/predicted_objects.json\", \"r\") as f:\n",
        "        predicted_objects_dataset= json.load(f)"
      ],
      "metadata": {
        "id": "iZgzxRJT3jni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_epoch(model, loader, optimizer=None, train=True, temperature=1.0):\n",
        "    mode = \"Training\" if train else \"Validation\"\n",
        "    model.train() if train else model.eval()\n",
        "    total_loss = 0\n",
        "    total_batches = len(loader)\n",
        "\n",
        "    for batch_idx, batch in enumerate(loader):\n",
        "        frame = batch['frame'].to(device)\n",
        "        target = batch['frame_caption']\n",
        "        video_folder = batch['video']\n",
        "        frame_name = batch['frame_number']\n",
        "\n",
        "        predicted_objects = [predicted_objects_dataset[video_folder[i]][frame_name[i]][\"predicted_objects\"]for i in range(len(video_folder)) ]\n",
        "\n",
        "\n",
        "        if train:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(frame, predicted_objects, target)\n",
        "            loss = outputs.loss / temperature\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            if scheduler:\n",
        "                scheduler.step()\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                outputs = model(frame, predicted_objects, target)\n",
        "                loss = outputs.loss / temperature\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Print progress bar\n",
        "        progress = (batch_idx + 1) / total_batches\n",
        "        bar_length = 20\n",
        "        filled_length = int(bar_length * progress)\n",
        "        bar = \"=\" * filled_length + \" \" * (bar_length - filled_length)\n",
        "        percentage = int(progress * 100)\n",
        "        sys.stdout.write(f\"\\r[{bar}] {percentage}% - Batch {batch_idx+1}/{total_batches} - Loss: {loss.item():.4f} - Avg {mode} Loss: {total_loss / (batch_idx + 1):.4f}\")\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    print()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "\n",
        "\n",
        "# Initialize\n",
        "model = FrameCaptioner().to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "num_epochs = 10\n",
        "temperature = 2.0\n",
        "num_training_steps = len(train_loader) * num_epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
        "\n",
        "model.load_state_dict(torch.load(f\"{save_dir}/model_FC.pth\"))\n",
        "\n",
        "\n",
        "# Train Loop\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}: \")\n",
        "    loss = run_epoch(model, train_loader, optimizer, train=True, temperature=temperature)\n",
        "    val_loss = run_epoch(model, val_loader, train=False, temperature=temperature)\n",
        "    torch.save(model.state_dict(), f\"{save_dir}/Models/model_FC_robust.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipM9IuDfaR9_",
        "outputId": "226e5405-86ae-4c78-f017-4c7a98c2c833"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-45-487bd90355cf>:57: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"/content/drive/My Drive/Master Thesis/CholecT50/model_FC.pth\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10: \n",
            "[====================] 100% - Batch 8983/8983 - Loss: 0.0221 - Avg Training Loss: 0.0144\n",
            "[====================] 100% - Batch 1123/1123 - Loss: 0.0037 - Avg Validation Loss: 0.0239\n",
            "Epoch 2/10: \n",
            "[====================] 100% - Batch 8983/8983 - Loss: 0.0048 - Avg Training Loss: 0.0106\n",
            "[====================] 100% - Batch 1123/1123 - Loss: 0.0021 - Avg Validation Loss: 0.0215\n",
            "Epoch 3/10: \n",
            "[====================] 100% - Batch 8983/8983 - Loss: 0.0035 - Avg Training Loss: 0.0088\n",
            "[====================] 100% - Batch 1123/1123 - Loss: 0.0029 - Avg Validation Loss: 0.0208\n",
            "Epoch 4/10: \n",
            "[====================] 100% - Batch 8983/8983 - Loss: 0.0009 - Avg Training Loss: 0.0073\n",
            "[====================] 100% - Batch 1123/1123 - Loss: 0.0016 - Avg Validation Loss: 0.0195\n",
            "Epoch 5/10: \n",
            "[====================] 100% - Batch 8983/8983 - Loss: 0.0083 - Avg Training Loss: 0.0060\n",
            "[====================] 100% - Batch 1123/1123 - Loss: 0.0018 - Avg Validation Loss: 0.0191\n",
            "Epoch 6/10: \n",
            "[====================] 100% - Batch 8983/8983 - Loss: 0.0034 - Avg Training Loss: 0.0049\n",
            "[====================] 100% - Batch 1123/1123 - Loss: 0.0024 - Avg Validation Loss: 0.0198\n",
            "Epoch 7/10: \n",
            "[====================] 100% - Batch 8983/8983 - Loss: 0.0008 - Avg Training Loss: 0.0039\n",
            "[====================] 100% - Batch 1123/1123 - Loss: 0.0013 - Avg Validation Loss: 0.0202\n",
            "Epoch 8/10: \n",
            "[====================] 100% - Batch 8983/8983 - Loss: 0.0003 - Avg Training Loss: 0.0030\n",
            "[====================] 100% - Batch 1123/1123 - Loss: 0.0005 - Avg Validation Loss: 0.0211\n",
            "Epoch 9/10: \n",
            "[====================] 100% - Batch 8983/8983 - Loss: 0.0014 - Avg Training Loss: 0.0022\n",
            "[====================] 100% - Batch 1123/1123 - Loss: 0.0001 - Avg Validation Loss: 0.0221\n",
            "Epoch 10/10: \n",
            "[====================] 100% - Batch 8983/8983 - Loss: 0.0018 - Avg Training Loss: 0.0016\n",
            "[====================] 100% - Batch 1123/1123 - Loss: 0.0001 - Avg Validation Loss: 0.0234\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare"
      ],
      "metadata": {
        "id": "6eYUEp7lR2i7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def evaluate_model(model, loader, device):\n",
        "    model.eval()\n",
        "    rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    total_batches = len(loader)\n",
        "    smooth_fn = SmoothingFunction().method1\n",
        "    rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
        "    bleu_scores = []\n",
        "    bert_precision, bert_recall, bert_f1 = [], [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(loader):\n",
        "            frame = batch['frame'].to(device)\n",
        "            target = batch['frame_caption']\n",
        "            video_folder = batch['video']\n",
        "            frame_name = batch['frame_number']\n",
        "\n",
        "\n",
        "\n",
        "            predicted_objects = [predicted_objects_dataset[video_folder[i]][frame_name[i]][\"predicted_objects\"]for i in range(len(video_folder)) ]\n",
        "            predicted_caption = model(frame, predicted_objects)\n",
        "\n",
        "            # Compute ROUGE scores\n",
        "            scores = rouge.score(predicted_caption, target)\n",
        "            for key in rouge_scores:\n",
        "                rouge_scores[key].append(scores[key].fmeasure)\n",
        "\n",
        "            # Compute BLEU score\n",
        "            reference = [target.split()]\n",
        "            hypothesis = predicted_caption.split()\n",
        "            bleu = sentence_bleu(reference, hypothesis, smoothing_function=smooth_fn)\n",
        "            bleu_scores.append(bleu)\n",
        "\n",
        "            # Compute BERTScore\n",
        "            logging.set_verbosity_error()\n",
        "            P, R, F1 = bert_score([predicted_caption], [target], lang=\"en\", rescale_with_baseline=True)\n",
        "            bert_precision.append(P.item())\n",
        "            bert_recall.append(R.item())\n",
        "            bert_f1.append(F1.item())\n",
        "\n",
        "\n",
        "            # Calculate progress\n",
        "            progress = (batch_idx + 1) / total_batches\n",
        "            bar_length = 20\n",
        "            filled_length = int(bar_length * progress)\n",
        "            bar = \"=\" * filled_length + \" \" * (bar_length - filled_length)\n",
        "            percentage = int(progress * 100)\n",
        "            sys.stdout.write(f\"\\r[{bar}] {percentage}% - Batch {batch_idx+1}/{total_batches}\")\n",
        "            sys.stdout.flush()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Compute average scores\n",
        "    avg_rouge1 = sum(rouge_scores['rouge1']) / len(rouge_scores['rouge1'])\n",
        "    avg_rouge2 = sum(rouge_scores['rouge2']) / len(rouge_scores['rouge2'])\n",
        "    avg_rougeL = sum(rouge_scores['rougeL']) / len(rouge_scores['rougeL'])\n",
        "    avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
        "    avg_bert_precision = sum(bert_precision) / len(bert_precision)\n",
        "    avg_bert_recall = sum(bert_recall) / len(bert_recall)\n",
        "    avg_bert_f1 = sum(bert_f1) / len(bert_f1)\n",
        "\n",
        "    print(\"\\nOverall Scores:\")\n",
        "    print(f\"Average BLEU: {avg_bleu:.4f}\")\n",
        "    print(f\"Average ROUGE-1: {avg_rouge1:.4f}\")\n",
        "    print(f\"Average ROUGE-2: {avg_rouge2:.4f}\")\n",
        "    print(f\"Average ROUGE-L: {avg_rougeL:.4f}\")\n",
        "    print(f\"Average BERT Precision: {avg_bert_precision:.4f}\")\n",
        "    print(f\"Average BERT Recall: {avg_bert_recall:.4f}\")\n",
        "    print(f\"Average BERT F1: {avg_bert_f1:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Before Robustness\n",
        "model = FrameCaptioner().to(device)\n",
        "model.load_state_dict(torch.load(f\"{save_dir}/Models/model_FC.pth\"))\n",
        "print(\"Results of frame captioner using Object detector before robustness: \")\n",
        "evaluate_model(model, test_loader, device)\n",
        "\n",
        "\n",
        "\n",
        "# After Robustness\n",
        "model = FrameCaptioner().to(device)\n",
        "model.load_state_dict(torch.load(f\"{save_dir}/Models/model_FC_robust.pth\"))\n",
        "print(\"Results of frame captioner using Object detector after robustness: \")\n",
        "evaluate_model(model, test_loader, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gq4tvLGOr2Kq",
        "outputId": "9d5d3700-cae9-4684-bfad-662951f45576"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-47-99f171cd7464>:86: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"/content/drive/My Drive/Master Thesis/CholecT50/final_FC.pth\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results of frame captioner using Object detector before robustness: \n",
            "[====================] 100% - Batch 1123/1123\n",
            "Overall Scores:\n",
            "Average BLEU: 0.6395\n",
            "Average ROUGE-1: 0.8351\n",
            "Average ROUGE-2: 0.7747\n",
            "Average ROUGE-L: 0.8116\n",
            "Average BERT Precision: 0.7771\n",
            "Average BERT Recall: 0.7644\n",
            "Average BERT F1: 0.7707\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-47-99f171cd7464>:94: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"/content/drive/My Drive/Master Thesis/CholecT50/final_FC_robust.pth\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results of frame captioner using Object detector after robustness: \n",
            "[====================] 100% - Batch 1123/1123\n",
            "Overall Scores:\n",
            "Average BLEU: 0.7267\n",
            "Average ROUGE-1: 0.8700\n",
            "Average ROUGE-2: 0.8096\n",
            "Average ROUGE-L: 0.8637\n",
            "Average BERT Precision: 0.7745\n",
            "Average BERT Recall: 0.8365\n",
            "Average BERT F1: 0.8052\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Store Generated captions\n",
        "\n",
        "Run this section to store the frame captions generated by the robust model"
      ],
      "metadata": {
        "id": "9w4RJ-PPGhH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frame_captioner = FrameCaptioner().to(device)\n",
        "frame_captioner.load_state_dict(torch.load(f\"{save_dir}/Models/model_FC_robust.pth\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4DlQbTDQ7Rw",
        "outputId": "f083d38a-767c-4269-a22e-bdbdeb7e6652"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-522481d1e28e>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  frame_captioner.load_state_dict(torch.load(\"/content/drive/My Drive/Master Thesis/CholecT50/final_FC_robust.pth\"))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "BM3vHM_SLSfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f\"{save_dir}/Predictions/predicted_objects.json\", \"r\") as f:\n",
        "        predicted_objects_dataset= json.load(f)"
      ],
      "metadata": {
        "id": "AvZRdxEA8WBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_dataset( FrameCaptioner, loader, device):\n",
        "    prediction_dataset = {}\n",
        "    total_batches = len(loader)\n",
        "\n",
        "\n",
        "\n",
        "    for batch_idx, batch in enumerate(loader):\n",
        "        with torch.no_grad():\n",
        "            frame = batch['frame'].to(device)\n",
        "            objects = batch['objects']\n",
        "            frame_caption = batch['frame_caption']\n",
        "            video_folder = batch['video']\n",
        "            frame_name = batch['frame_number']\n",
        "\n",
        "            predicted_objects = [predicted_objects_dataset[video_folder[i]][frame_name[i]][\"predicted_objects\"]for i in range(len(video_folder)) ]\n",
        "            predicted_caption = FrameCaptioner(frame, predicted_objects)\n",
        "\n",
        "\n",
        "            for i in range(len(video_folder)):\n",
        "                vid = video_folder[i]\n",
        "                frm = frame_name[i]\n",
        "\n",
        "                if vid not in prediction_dataset:\n",
        "                    prediction_dataset[vid] = {}\n",
        "\n",
        "                prediction_dataset[vid][frm] = {\n",
        "                    \"predicted_caption\": predicted_caption[i]\n",
        "                }\n",
        "\n",
        "            # Print progress bar\n",
        "            progress = (batch_idx + 1) / total_batches\n",
        "            bar_length = 20\n",
        "            filled_length = int(bar_length * progress)\n",
        "            bar = \"=\" * filled_length + \" \" * (bar_length - filled_length)\n",
        "            percentage = int(progress * 100)\n",
        "            sys.stdout.write(f\"\\r[{bar}] {percentage}% - Batch {batch_idx+1}/{total_batches}\")\n",
        "            sys.stdout.flush()\n",
        "\n",
        "    return prediction_dataset\n",
        "\n",
        "prediction_dataset = predict_dataset(frame_captioner, loader, device)\n",
        "\n",
        "# Save to JSON file\n",
        "save_path = f\"{save_dir}/Predictions/predicted_frames.json\"\n",
        "with open(save_path, \"w\") as f:\n",
        "    json.dump(prediction_dataset, f, indent=4)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRZV7eXvXqL4",
        "outputId": "00946c1f-e479-411e-e656-8a0850b3dd25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[====================] 100% - Batch 11229/11229"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}